{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8842b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F \n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3012953b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Setting up hyperparameters and device ---\n",
      "Using device: cuda:0\n",
      "\n",
      "--- Preparing CIFAR-10 dataset ---\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "\n",
      "--- Epoch 1/20 ---\n",
      "Epoch Summary | Loss: 1.7751 | Accuracy: 35.31%\n",
      "\n",
      "--- Epoch 2/20 ---\n",
      "Epoch Summary | Loss: 1.4656 | Accuracy: 45.93%\n",
      "\n",
      "--- Epoch 3/20 ---\n",
      "Epoch Summary | Loss: 1.3420 | Accuracy: 51.06%\n",
      "\n",
      "--- Epoch 4/20 ---\n",
      "Epoch Summary | Loss: 1.2499 | Accuracy: 54.81%\n",
      "\n",
      "--- Epoch 5/20 ---\n",
      "Epoch Summary | Loss: 1.1885 | Accuracy: 57.31%\n",
      "\n",
      "--- Epoch 6/20 ---\n",
      "Epoch Summary | Loss: 1.1433 | Accuracy: 58.97%\n",
      "\n",
      "--- Epoch 7/20 ---\n",
      "Epoch Summary | Loss: 1.1039 | Accuracy: 60.74%\n",
      "\n",
      "--- Epoch 8/20 ---\n",
      "Epoch Summary | Loss: 1.0767 | Accuracy: 61.54%\n",
      "\n",
      "--- Epoch 9/20 ---\n",
      "Epoch Summary | Loss: 1.0430 | Accuracy: 62.69%\n",
      "\n",
      "--- Epoch 10/20 ---\n",
      "Epoch Summary | Loss: 1.0225 | Accuracy: 63.75%\n",
      "\n",
      "--- Epoch 11/20 ---\n",
      "Epoch Summary | Loss: 1.0043 | Accuracy: 64.33%\n",
      "\n",
      "--- Epoch 12/20 ---\n",
      "Epoch Summary | Loss: 0.9852 | Accuracy: 65.23%\n",
      "\n",
      "--- Epoch 13/20 ---\n",
      "Epoch Summary | Loss: 0.9631 | Accuracy: 66.26%\n",
      "\n",
      "--- Epoch 14/20 ---\n",
      "Epoch Summary | Loss: 0.9510 | Accuracy: 66.69%\n",
      "\n",
      "--- Epoch 15/20 ---\n",
      "Epoch Summary | Loss: 0.9368 | Accuracy: 67.35%\n",
      "\n",
      "--- Epoch 16/20 ---\n",
      "Epoch Summary | Loss: 0.9200 | Accuracy: 67.91%\n",
      "\n",
      "--- Epoch 17/20 ---\n",
      "Epoch Summary | Loss: 0.9094 | Accuracy: 68.11%\n",
      "\n",
      "--- Epoch 18/20 ---\n",
      "Epoch Summary | Loss: 0.9002 | Accuracy: 68.54%\n",
      "\n",
      "--- Epoch 19/20 ---\n",
      "Epoch Summary | Loss: 0.8875 | Accuracy: 68.95%\n",
      "\n",
      "--- Epoch 20/20 ---\n",
      "Epoch Summary | Loss: 0.8798 | Accuracy: 69.22%\n",
      "\n",
      "--- Finished Training ---\n",
      "\n",
      "Test Loss: 0.7691 | Test Accuracy: 73.51%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.7690620652198792, 73.50999999999999)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ======================================================================\n",
    "# CUSTOM CNN on CIFAR-10 (PyTorch) (256 batch size)\n",
    "# ======================================================================\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F \n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# ======================================================================\n",
    "# SECTION 1: SETUP AND HYPERPARAMETERS\n",
    "# ======================================================================\n",
    "print(\"--- Setting up hyperparameters and device ---\")\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Hyperparameters\n",
    "EPOCHS = 20\n",
    "BATCH_SIZE = 256\n",
    "LEARNING_RATE = 0.001\n",
    "NUM_CLASSES = 10           # CIFAR-10 has 10 classes\n",
    "IMAGE_SIZE = 64            # Resize from 32x32 → 64x64\n",
    "\n",
    "# ======================================================================\n",
    "# SECTION 2: DATA LOADING AND AUGMENTATION\n",
    "# ======================================================================\n",
    "print(\"\\n--- Preparing CIFAR-10 dataset ---\")\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize(IMAGE_SIZE),\n",
    "    transforms.RandomCrop(IMAGE_SIZE, padding=8),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
    "                         (0.2023, 0.1994, 0.2010))\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize(IMAGE_SIZE),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
    "                         (0.2023, 0.1994, 0.2010))\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(\n",
    "    root='./data', train=True, download=True, transform=train_transform)\n",
    "testset = torchvision.datasets.CIFAR10(\n",
    "    root='./data', train=False, download=True, transform=test_transform)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "    trainset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "    testset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "\n",
    "# ======================================================================\n",
    "# SECTION 3: CUSTOM CNN MODEL DEFINITION\n",
    "# ======================================================================\n",
    "\n",
    "class CustomCNN(nn.Module):\n",
    "    \"\"\"Custom CNN architecture for CIFAR-10 (64x64).\"\"\"\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(CustomCNN, self).__init__()\n",
    "        \n",
    "        # Block 1: Conv -> BN -> ReLU -> MaxPool\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2)\n",
    "        )\n",
    "\n",
    "        # Block 2\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(16, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2)\n",
    "        )\n",
    "\n",
    "        # Block 3\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2)\n",
    "        )\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(64 * 8 * 8, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "# ======================================================================\n",
    "# SECTION 4: TRAINING AND EVALUATION FUNCTIONS\n",
    "# ======================================================================\n",
    "\n",
    "def train_model(model, trainloader, criterion, optimizer, epochs=10, device='cpu'):\n",
    "    \"\"\"Train a PyTorch model.\"\"\"\n",
    "    history = {'loss': [], 'acc': []}\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct_predictions = 0\n",
    "        total_samples = 0\n",
    "        \n",
    "        print(f\"\\n--- Epoch {epoch+1}/{epochs} ---\")\n",
    "        for i, (inputs, labels) in enumerate(trainloader, 0):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_samples += labels.size(0)\n",
    "            correct_predictions += (predicted == labels).sum().item()\n",
    "\n",
    "        epoch_loss = running_loss / total_samples\n",
    "        epoch_acc = (correct_predictions / total_samples) * 100\n",
    "        history['loss'].append(epoch_loss)\n",
    "        history['acc'].append(epoch_acc)\n",
    "        print(f\"Epoch Summary | Loss: {epoch_loss:.4f} | Accuracy: {epoch_acc:.2f}%\")\n",
    "    \n",
    "    print('\\n--- Finished Training ---')\n",
    "    return history\n",
    "\n",
    "\n",
    "def evaluate_model(model, testloader, criterion, device='cpu'):\n",
    "    \"\"\"Evaluate model performance on test set.\"\"\"\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in testloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_samples += labels.size(0)\n",
    "            correct_predictions += (predicted == labels).sum().item()\n",
    "\n",
    "    test_loss = running_loss / total_samples\n",
    "    test_acc = (correct_predictions / total_samples) * 100\n",
    "    print(f\"\\nTest Loss: {test_loss:.4f} | Test Accuracy: {test_acc:.2f}%\")\n",
    "    return test_loss, test_acc\n",
    "\n",
    "# ======================================================================\n",
    "# SECTION 5: TRAIN + EVALUATE PIPELINE\n",
    "# ======================================================================\n",
    "\n",
    "model = CustomCNN(num_classes=NUM_CLASSES).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "train_history = train_model(model, trainloader, criterion, optimizer,\n",
    "                            epochs=EPOCHS, device=device)\n",
    "evaluate_model(model, testloader, criterion, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a88703fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Setting up hyperparameters and device ---\n",
      "Using device: cuda:0\n",
      "\n",
      "--- Preparing CIFAR-10 dataset ---\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "\n",
      "--- Epoch 1/20 ---\n",
      "Epoch Summary | Loss: 2.3201 | Accuracy: 22.74%\n",
      "\n",
      "--- Epoch 2/20 ---\n",
      "Epoch Summary | Loss: 1.8456 | Accuracy: 29.40%\n",
      "\n",
      "--- Epoch 3/20 ---\n",
      "Epoch Summary | Loss: 1.7491 | Accuracy: 33.13%\n",
      "\n",
      "--- Epoch 4/20 ---\n",
      "Epoch Summary | Loss: 1.6814 | Accuracy: 36.43%\n",
      "\n",
      "--- Epoch 5/20 ---\n",
      "Epoch Summary | Loss: 1.6373 | Accuracy: 37.86%\n",
      "\n",
      "--- Epoch 6/20 ---\n",
      "Epoch Summary | Loss: 1.5992 | Accuracy: 39.38%\n",
      "\n",
      "--- Epoch 7/20 ---\n",
      "Epoch Summary | Loss: 1.5587 | Accuracy: 41.11%\n",
      "\n",
      "--- Epoch 8/20 ---\n",
      "Epoch Summary | Loss: 1.5268 | Accuracy: 42.82%\n",
      "\n",
      "--- Epoch 9/20 ---\n",
      "Epoch Summary | Loss: 1.4994 | Accuracy: 44.25%\n",
      "\n",
      "--- Epoch 10/20 ---\n",
      "Epoch Summary | Loss: 1.4771 | Accuracy: 45.33%\n",
      "\n",
      "--- Epoch 11/20 ---\n",
      "Epoch Summary | Loss: 1.4538 | Accuracy: 45.69%\n",
      "\n",
      "--- Epoch 12/20 ---\n",
      "Epoch Summary | Loss: 1.4323 | Accuracy: 46.92%\n",
      "\n",
      "--- Epoch 13/20 ---\n",
      "Epoch Summary | Loss: 1.4150 | Accuracy: 47.68%\n",
      "\n",
      "--- Epoch 14/20 ---\n",
      "Epoch Summary | Loss: 1.4018 | Accuracy: 48.46%\n",
      "\n",
      "--- Epoch 15/20 ---\n",
      "Epoch Summary | Loss: 1.3912 | Accuracy: 48.73%\n",
      "\n",
      "--- Epoch 16/20 ---\n",
      "Epoch Summary | Loss: 1.3713 | Accuracy: 49.43%\n",
      "\n",
      "--- Epoch 17/20 ---\n",
      "Epoch Summary | Loss: 1.3571 | Accuracy: 50.09%\n",
      "\n",
      "--- Epoch 18/20 ---\n",
      "Epoch Summary | Loss: 1.3406 | Accuracy: 50.78%\n",
      "\n",
      "--- Epoch 19/20 ---\n",
      "Epoch Summary | Loss: 1.3187 | Accuracy: 51.69%\n",
      "\n",
      "--- Epoch 20/20 ---\n",
      "Epoch Summary | Loss: 1.3144 | Accuracy: 51.30%\n",
      "\n",
      "--- Finished Training ---\n",
      "\n",
      "Test Loss: 1.1127 | Test Accuracy: 61.02%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1.1126872608184815, 61.019999999999996)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ======================================================================\n",
    "# CUSTOM CNN on CIFAR-10 (PyTorch) (256 batch size)\n",
    "# ======================================================================\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F \n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# ======================================================================\n",
    "# SECTION 1: SETUP AND HYPERPARAMETERS\n",
    "# ======================================================================\n",
    "print(\"--- Setting up hyperparameters and device ---\")\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Hyperparameters\n",
    "EPOCHS = 20\n",
    "BATCH_SIZE = 256\n",
    "LEARNING_RATE = 0.005\n",
    "NUM_CLASSES = 10           # CIFAR-10 has 10 classes\n",
    "IMAGE_SIZE = 64            # Resize from 32x32 → 64x64\n",
    "\n",
    "# ======================================================================\n",
    "# SECTION 2: DATA LOADING AND AUGMENTATION\n",
    "# ======================================================================\n",
    "print(\"\\n--- Preparing CIFAR-10 dataset ---\")\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize(IMAGE_SIZE),\n",
    "    transforms.RandomCrop(IMAGE_SIZE, padding=8),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
    "                         (0.2023, 0.1994, 0.2010))\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize(IMAGE_SIZE),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
    "                         (0.2023, 0.1994, 0.2010))\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(\n",
    "    root='./data', train=True, download=True, transform=train_transform)\n",
    "testset = torchvision.datasets.CIFAR10(\n",
    "    root='./data', train=False, download=True, transform=test_transform)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "    trainset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "    testset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "\n",
    "# ======================================================================\n",
    "# SECTION 3: CUSTOM CNN MODEL DEFINITION\n",
    "# ======================================================================\n",
    "\n",
    "class CustomCNN(nn.Module):\n",
    "    \"\"\"Custom CNN architecture for CIFAR-10 (64x64).\"\"\"\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(CustomCNN, self).__init__()\n",
    "        \n",
    "        # Block 1: Conv -> BN -> ReLU -> MaxPool\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2)\n",
    "        )\n",
    "\n",
    "        # Block 2\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(16, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2)\n",
    "        )\n",
    "\n",
    "        # Block 3\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2)\n",
    "        )\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(64 * 8 * 8, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "# ======================================================================\n",
    "# SECTION 4: TRAINING AND EVALUATION FUNCTIONS\n",
    "# ======================================================================\n",
    "\n",
    "def train_model(model, trainloader, criterion, optimizer, epochs=10, device='cpu'):\n",
    "    \"\"\"Train a PyTorch model.\"\"\"\n",
    "    history = {'loss': [], 'acc': []}\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct_predictions = 0\n",
    "        total_samples = 0\n",
    "        \n",
    "        print(f\"\\n--- Epoch {epoch+1}/{epochs} ---\")\n",
    "        for i, (inputs, labels) in enumerate(trainloader, 0):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_samples += labels.size(0)\n",
    "            correct_predictions += (predicted == labels).sum().item()\n",
    "\n",
    "        epoch_loss = running_loss / total_samples\n",
    "        epoch_acc = (correct_predictions / total_samples) * 100\n",
    "        history['loss'].append(epoch_loss)\n",
    "        history['acc'].append(epoch_acc)\n",
    "        print(f\"Epoch Summary | Loss: {epoch_loss:.4f} | Accuracy: {epoch_acc:.2f}%\")\n",
    "    \n",
    "    print('\\n--- Finished Training ---')\n",
    "    return history\n",
    "\n",
    "\n",
    "def evaluate_model(model, testloader, criterion, device='cpu'):\n",
    "    \"\"\"Evaluate model performance on test set.\"\"\"\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in testloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_samples += labels.size(0)\n",
    "            correct_predictions += (predicted == labels).sum().item()\n",
    "\n",
    "    test_loss = running_loss / total_samples\n",
    "    test_acc = (correct_predictions / total_samples) * 100\n",
    "    print(f\"\\nTest Loss: {test_loss:.4f} | Test Accuracy: {test_acc:.2f}%\")\n",
    "    return test_loss, test_acc\n",
    "\n",
    "# ======================================================================\n",
    "# SECTION 5: TRAIN + EVALUATE PIPELINE\n",
    "# ======================================================================\n",
    "\n",
    "model = CustomCNN(num_classes=NUM_CLASSES).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "train_history = train_model(model, trainloader, criterion, optimizer,\n",
    "                            epochs=EPOCHS, device=device)\n",
    "evaluate_model(model, testloader, criterion, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf465769",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Setting up hyperparameters and device ---\n",
      "Using device: cuda:0\n",
      "\n",
      "--- Preparing CIFAR-10 dataset ---\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "\n",
      "--- Epoch 1/20 ---\n",
      "Epoch Summary | Loss: 1.6751 | Accuracy: 38.54%\n",
      "\n",
      "--- Epoch 2/20 ---\n",
      "Epoch Summary | Loss: 1.4081 | Accuracy: 48.67%\n",
      "\n",
      "--- Epoch 3/20 ---\n",
      "Epoch Summary | Loss: 1.3022 | Accuracy: 52.82%\n",
      "\n",
      "--- Epoch 4/20 ---\n",
      "Epoch Summary | Loss: 1.2198 | Accuracy: 56.33%\n",
      "\n",
      "--- Epoch 5/20 ---\n",
      "Epoch Summary | Loss: 1.1669 | Accuracy: 58.04%\n",
      "\n",
      "--- Epoch 6/20 ---\n",
      "Epoch Summary | Loss: 1.1097 | Accuracy: 60.53%\n",
      "\n",
      "--- Epoch 7/20 ---\n",
      "Epoch Summary | Loss: 1.0783 | Accuracy: 61.45%\n",
      "\n",
      "--- Epoch 8/20 ---\n",
      "Epoch Summary | Loss: 1.0335 | Accuracy: 63.33%\n",
      "\n",
      "--- Epoch 9/20 ---\n",
      "Epoch Summary | Loss: 1.0054 | Accuracy: 64.44%\n",
      "\n",
      "--- Epoch 10/20 ---\n",
      "Epoch Summary | Loss: 0.9948 | Accuracy: 64.55%\n",
      "\n",
      "--- Epoch 11/20 ---\n",
      "Epoch Summary | Loss: 0.9594 | Accuracy: 65.97%\n",
      "\n",
      "--- Epoch 12/20 ---\n",
      "Epoch Summary | Loss: 0.9439 | Accuracy: 66.71%\n",
      "\n",
      "--- Epoch 13/20 ---\n",
      "Epoch Summary | Loss: 0.9307 | Accuracy: 67.22%\n",
      "\n",
      "--- Epoch 14/20 ---\n",
      "Epoch Summary | Loss: 0.9121 | Accuracy: 67.91%\n",
      "\n",
      "--- Epoch 15/20 ---\n",
      "Epoch Summary | Loss: 0.9018 | Accuracy: 68.34%\n",
      "\n",
      "--- Epoch 16/20 ---\n",
      "Epoch Summary | Loss: 0.8829 | Accuracy: 68.96%\n",
      "\n",
      "--- Epoch 17/20 ---\n",
      "Epoch Summary | Loss: 0.8725 | Accuracy: 69.32%\n",
      "\n",
      "--- Epoch 18/20 ---\n",
      "Epoch Summary | Loss: 0.8593 | Accuracy: 69.88%\n",
      "\n",
      "--- Epoch 19/20 ---\n",
      "Epoch Summary | Loss: 0.8454 | Accuracy: 70.42%\n",
      "\n",
      "--- Epoch 20/20 ---\n",
      "Epoch Summary | Loss: 0.8439 | Accuracy: 70.51%\n",
      "\n",
      "--- Finished Training ---\n",
      "\n",
      "Test Loss: 0.7220 | Test Accuracy: 74.96%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.7219692619323731, 74.96000000000001)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ======================================================================\n",
    "# CUSTOM CNN on CIFAR-10 (PyTorch) (256 batch size)\n",
    "# ======================================================================\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F \n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# ======================================================================\n",
    "# SECTION 1: SETUP AND HYPERPARAMETERS\n",
    "# ======================================================================\n",
    "print(\"--- Setting up hyperparameters and device ---\")\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Hyperparameters\n",
    "EPOCHS = 20\n",
    "BATCH_SIZE = 256\n",
    "LEARNING_RATE = 0.0005\n",
    "NUM_CLASSES = 10           # CIFAR-10 has 10 classes\n",
    "IMAGE_SIZE = 64            # Resize from 32x32 → 64x64\n",
    "\n",
    "# ======================================================================\n",
    "# SECTION 2: DATA LOADING AND AUGMENTATION\n",
    "# ======================================================================\n",
    "print(\"\\n--- Preparing CIFAR-10 dataset ---\")\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize(IMAGE_SIZE),\n",
    "    transforms.RandomCrop(IMAGE_SIZE, padding=8),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
    "                         (0.2023, 0.1994, 0.2010))\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize(IMAGE_SIZE),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
    "                         (0.2023, 0.1994, 0.2010))\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(\n",
    "    root='./data', train=True, download=True, transform=train_transform)\n",
    "testset = torchvision.datasets.CIFAR10(\n",
    "    root='./data', train=False, download=True, transform=test_transform)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "    trainset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "    testset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "\n",
    "# ======================================================================\n",
    "# SECTION 3: CUSTOM CNN MODEL DEFINITION\n",
    "# ======================================================================\n",
    "\n",
    "class CustomCNN(nn.Module):\n",
    "    \"\"\"Custom CNN architecture for CIFAR-10 (64x64).\"\"\"\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(CustomCNN, self).__init__()\n",
    "        \n",
    "        # Block 1: Conv -> BN -> ReLU -> MaxPool\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2)\n",
    "        )\n",
    "\n",
    "        # Block 2\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(16, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2)\n",
    "        )\n",
    "\n",
    "        # Block 3\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2)\n",
    "        )\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(64 * 8 * 8, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "# ======================================================================\n",
    "# SECTION 4: TRAINING AND EVALUATION FUNCTIONS\n",
    "# ======================================================================\n",
    "\n",
    "def train_model(model, trainloader, criterion, optimizer, epochs=10, device='cpu'):\n",
    "    \"\"\"Train a PyTorch model.\"\"\"\n",
    "    history = {'loss': [], 'acc': []}\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct_predictions = 0\n",
    "        total_samples = 0\n",
    "        \n",
    "        print(f\"\\n--- Epoch {epoch+1}/{epochs} ---\")\n",
    "        for i, (inputs, labels) in enumerate(trainloader, 0):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_samples += labels.size(0)\n",
    "            correct_predictions += (predicted == labels).sum().item()\n",
    "\n",
    "        epoch_loss = running_loss / total_samples\n",
    "        epoch_acc = (correct_predictions / total_samples) * 100\n",
    "        history['loss'].append(epoch_loss)\n",
    "        history['acc'].append(epoch_acc)\n",
    "        print(f\"Epoch Summary | Loss: {epoch_loss:.4f} | Accuracy: {epoch_acc:.2f}%\")\n",
    "    \n",
    "    print('\\n--- Finished Training ---')\n",
    "    return history\n",
    "\n",
    "\n",
    "def evaluate_model(model, testloader, criterion, device='cpu'):\n",
    "    \"\"\"Evaluate model performance on test set.\"\"\"\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in testloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_samples += labels.size(0)\n",
    "            correct_predictions += (predicted == labels).sum().item()\n",
    "\n",
    "    test_loss = running_loss / total_samples\n",
    "    test_acc = (correct_predictions / total_samples) * 100\n",
    "    print(f\"\\nTest Loss: {test_loss:.4f} | Test Accuracy: {test_acc:.2f}%\")\n",
    "    return test_loss, test_acc\n",
    "\n",
    "# ======================================================================\n",
    "# SECTION 5: TRAIN + EVALUATE PIPELINE\n",
    "# ======================================================================\n",
    "\n",
    "model = CustomCNN(num_classes=NUM_CLASSES).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "train_history = train_model(model, trainloader, criterion, optimizer,\n",
    "                            epochs=EPOCHS, device=device)\n",
    "evaluate_model(model, testloader, criterion, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faefaa4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Epoch 1/20 ---\n",
      "Epoch Summary | Loss: 1.6866 | Accuracy: 38.15%\n",
      "\n",
      "--- Epoch 2/20 ---\n",
      "Epoch Summary | Loss: 1.4022 | Accuracy: 48.84%\n",
      "\n",
      "--- Epoch 3/20 ---\n",
      "Epoch Summary | Loss: 1.2644 | Accuracy: 54.04%\n",
      "\n",
      "--- Epoch 4/20 ---\n",
      "Epoch Summary | Loss: 1.1520 | Accuracy: 58.57%\n",
      "\n",
      "--- Epoch 5/20 ---\n",
      "Epoch Summary | Loss: 1.0859 | Accuracy: 61.31%\n",
      "\n",
      "--- Epoch 6/20 ---\n",
      "Epoch Summary | Loss: 1.0294 | Accuracy: 63.68%\n",
      "\n",
      "--- Epoch 7/20 ---\n",
      "Epoch Summary | Loss: 0.9774 | Accuracy: 65.27%\n",
      "\n",
      "--- Epoch 8/20 ---\n",
      "Epoch Summary | Loss: 0.9373 | Accuracy: 66.67%\n",
      "\n",
      "--- Epoch 9/20 ---\n",
      "Epoch Summary | Loss: 0.9041 | Accuracy: 67.87%\n",
      "\n",
      "--- Epoch 10/20 ---\n",
      "Epoch Summary | Loss: 0.8773 | Accuracy: 68.88%\n",
      "\n",
      "--- Epoch 11/20 ---\n",
      "Epoch Summary | Loss: 0.8479 | Accuracy: 69.98%\n",
      "\n",
      "--- Epoch 12/20 ---\n",
      "Epoch Summary | Loss: 0.8265 | Accuracy: 71.02%\n",
      "\n",
      "--- Epoch 13/20 ---\n",
      "Epoch Summary | Loss: 0.8071 | Accuracy: 71.55%\n",
      "\n",
      "--- Epoch 14/20 ---\n",
      "Epoch Summary | Loss: 0.7835 | Accuracy: 72.55%\n",
      "\n",
      "--- Epoch 15/20 ---\n",
      "Epoch Summary | Loss: 0.7689 | Accuracy: 72.98%\n",
      "\n",
      "--- Epoch 16/20 ---\n",
      "Epoch Summary | Loss: 0.7555 | Accuracy: 73.51%\n",
      "\n",
      "--- Epoch 17/20 ---\n",
      "Epoch Summary | Loss: 0.7375 | Accuracy: 74.21%\n",
      "\n",
      "--- Epoch 18/20 ---\n",
      "Epoch Summary | Loss: 0.7311 | Accuracy: 74.38%\n",
      "\n",
      "--- Epoch 19/20 ---\n",
      "Epoch Summary | Loss: 0.7167 | Accuracy: 75.02%\n",
      "\n",
      "--- Epoch 20/20 ---\n",
      "Epoch Summary | Loss: 0.7116 | Accuracy: 75.19%\n",
      "\n",
      "--- Finished Training ---\n",
      "\n",
      "Test Loss: 0.6450 | Test Accuracy: 77.92%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.6450141662597656, 77.92)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Hyperparameters\n",
    "EPOCHS = 20\n",
    "BATCH_SIZE = 256\n",
    "LEARNING_RATE = 0.001\n",
    "NUM_CLASSES = 10           # CIFAR-10 has 10 classes\n",
    "IMAGE_SIZE = 64            # Resize from 32x32 → 64x64\n",
    "# ======================================================================\n",
    "# SECTION 5: TRAIN + EVALUATE PIPELINE\n",
    "# ======================================================================\n",
    "\n",
    "model = CustomCNN(num_classes=NUM_CLASSES).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9, weight_decay=5e-4)\n",
    "\n",
    "\n",
    "train_history = train_model(model, trainloader, criterion, optimizer,\n",
    "                            epochs=EPOCHS, device=device)\n",
    "evaluate_model(model, testloader, criterion, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "abab038f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Epoch 1/20 ---\n",
      "Epoch Summary | Loss: 1.9742 | Accuracy: 27.70%\n",
      "\n",
      "--- Epoch 2/20 ---\n",
      "Epoch Summary | Loss: 1.6561 | Accuracy: 39.01%\n",
      "\n",
      "--- Epoch 3/20 ---\n",
      "Epoch Summary | Loss: 1.5276 | Accuracy: 43.76%\n",
      "\n",
      "--- Epoch 4/20 ---\n",
      "Epoch Summary | Loss: 1.4615 | Accuracy: 46.71%\n",
      "\n",
      "--- Epoch 5/20 ---\n",
      "Epoch Summary | Loss: 1.4137 | Accuracy: 48.68%\n",
      "\n",
      "--- Epoch 6/20 ---\n",
      "Epoch Summary | Loss: 1.3698 | Accuracy: 50.28%\n",
      "\n",
      "--- Epoch 7/20 ---\n",
      "Epoch Summary | Loss: 1.3334 | Accuracy: 52.06%\n",
      "\n",
      "--- Epoch 8/20 ---\n",
      "Epoch Summary | Loss: 1.2956 | Accuracy: 53.70%\n",
      "\n",
      "--- Epoch 9/20 ---\n",
      "Epoch Summary | Loss: 1.2587 | Accuracy: 54.78%\n",
      "\n",
      "--- Epoch 10/20 ---\n",
      "Epoch Summary | Loss: 1.2234 | Accuracy: 56.08%\n",
      "\n",
      "--- Epoch 11/20 ---\n",
      "Epoch Summary | Loss: 1.1885 | Accuracy: 57.65%\n",
      "\n",
      "--- Epoch 12/20 ---\n",
      "Epoch Summary | Loss: 1.1785 | Accuracy: 58.11%\n",
      "\n",
      "--- Epoch 13/20 ---\n",
      "Epoch Summary | Loss: 1.1731 | Accuracy: 58.20%\n",
      "\n",
      "--- Epoch 14/20 ---\n",
      "Epoch Summary | Loss: 1.1694 | Accuracy: 58.43%\n",
      "\n",
      "--- Epoch 15/20 ---\n",
      "Epoch Summary | Loss: 1.1693 | Accuracy: 58.27%\n",
      "\n",
      "--- Epoch 16/20 ---\n",
      "Epoch Summary | Loss: 1.1607 | Accuracy: 58.76%\n",
      "\n",
      "--- Epoch 17/20 ---\n",
      "Epoch Summary | Loss: 1.1626 | Accuracy: 58.28%\n",
      "\n",
      "--- Epoch 18/20 ---\n",
      "Epoch Summary | Loss: 1.1496 | Accuracy: 59.27%\n",
      "\n",
      "--- Epoch 19/20 ---\n",
      "Epoch Summary | Loss: 1.1580 | Accuracy: 58.91%\n",
      "\n",
      "--- Epoch 20/20 ---\n",
      "Epoch Summary | Loss: 1.1515 | Accuracy: 59.00%\n",
      "\n",
      "--- Finished Training ---\n",
      "\n",
      "Test Loss: 1.0293 | Test Accuracy: 63.40%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1.0292995738983155, 63.4)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ======================================================================\n",
    "# SECTION 5: TRAIN + EVALUATE PIPELINE (SGD + Scheduler, batch=256)\n",
    "# ======================================================================\n",
    "\n",
    "BATCH_SIZE = 256  # Update batch size\n",
    "\n",
    "# Re-create DataLoaders with new batch size\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "    trainset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "    testset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "\n",
    "# Model\n",
    "model = CustomCNN(num_classes=NUM_CLASSES).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# SGD optimizer with momentum and weight decay\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9, weight_decay=5e-4)\n",
    "\n",
    "# StepLR scheduler: reduce LR by 0.1 every 10 epochs\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "scheduler = StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "\n",
    "# Training function with scheduler\n",
    "def train_model_with_scheduler(model, trainloader, criterion, optimizer, scheduler, epochs=10, device='cpu'):\n",
    "    history = {'loss': [], 'acc': []}\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct_predictions = 0\n",
    "        total_samples = 0\n",
    "        \n",
    "        print(f\"\\n--- Epoch {epoch+1}/{epochs} ---\")\n",
    "        for i, (inputs, labels) in enumerate(trainloader, 0):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_samples += labels.size(0)\n",
    "            correct_predictions += (predicted == labels).sum().item()\n",
    "\n",
    "        # Step the scheduler at the end of each epoch\n",
    "        scheduler.step()\n",
    "\n",
    "        epoch_loss = running_loss / total_samples\n",
    "        epoch_acc = (correct_predictions / total_samples) * 100\n",
    "        history['loss'].append(epoch_loss)\n",
    "        history['acc'].append(epoch_acc)\n",
    "        print(f\"Epoch Summary | Loss: {epoch_loss:.4f} | Accuracy: {epoch_acc:.2f}%\")\n",
    "    \n",
    "    print('\\n--- Finished Training ---')\n",
    "    return history\n",
    "\n",
    "# Train with scheduler\n",
    "train_history = train_model_with_scheduler(model, trainloader, criterion, optimizer, scheduler,\n",
    "                                           epochs=20, device=device)\n",
    "\n",
    "# Evaluate model\n",
    "evaluate_model(model, testloader, criterion, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf289ae6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Epoch 1/20 ---\n",
      "Epoch Summary | Loss: 1.7536 | Accuracy: 35.31%\n",
      "\n",
      "--- Epoch 2/20 ---\n",
      "Epoch Summary | Loss: 1.4960 | Accuracy: 45.07%\n",
      "\n",
      "--- Epoch 3/20 ---\n",
      "Epoch Summary | Loss: 1.2910 | Accuracy: 53.45%\n",
      "\n",
      "--- Epoch 4/20 ---\n",
      "Epoch Summary | Loss: 1.1791 | Accuracy: 58.02%\n",
      "\n",
      "--- Epoch 5/20 ---\n",
      "Epoch Summary | Loss: 1.0859 | Accuracy: 61.60%\n",
      "\n",
      "--- Epoch 6/20 ---\n",
      "Epoch Summary | Loss: 1.0212 | Accuracy: 64.15%\n",
      "\n",
      "--- Epoch 7/20 ---\n",
      "Epoch Summary | Loss: 0.9776 | Accuracy: 65.64%\n",
      "\n",
      "--- Epoch 8/20 ---\n",
      "Epoch Summary | Loss: 0.9385 | Accuracy: 67.00%\n",
      "\n",
      "--- Epoch 9/20 ---\n",
      "Epoch Summary | Loss: 0.9119 | Accuracy: 68.24%\n",
      "\n",
      "--- Epoch 10/20 ---\n",
      "Epoch Summary | Loss: 0.8827 | Accuracy: 69.08%\n",
      "\n",
      "--- Epoch 11/20 ---\n",
      "Epoch Summary | Loss: 0.7840 | Accuracy: 72.79%\n",
      "\n",
      "--- Epoch 12/20 ---\n",
      "Epoch Summary | Loss: 0.7513 | Accuracy: 74.01%\n",
      "\n",
      "--- Epoch 13/20 ---\n",
      "Epoch Summary | Loss: 0.7474 | Accuracy: 74.10%\n",
      "\n",
      "--- Epoch 14/20 ---\n",
      "Epoch Summary | Loss: 0.7292 | Accuracy: 74.64%\n",
      "\n",
      "--- Epoch 15/20 ---\n",
      "Epoch Summary | Loss: 0.7265 | Accuracy: 74.89%\n",
      "\n",
      "--- Epoch 16/20 ---\n",
      "Epoch Summary | Loss: 0.7165 | Accuracy: 75.31%\n",
      "\n",
      "--- Epoch 17/20 ---\n",
      "Epoch Summary | Loss: 0.7143 | Accuracy: 75.05%\n",
      "\n",
      "--- Epoch 18/20 ---\n",
      "Epoch Summary | Loss: 0.7095 | Accuracy: 75.30%\n",
      "\n",
      "--- Epoch 19/20 ---\n",
      "Epoch Summary | Loss: 0.7050 | Accuracy: 75.52%\n",
      "\n",
      "--- Epoch 20/20 ---\n",
      "Epoch Summary | Loss: 0.7016 | Accuracy: 75.67%\n",
      "\n",
      "--- Finished Training ---\n",
      "\n",
      "Test Loss: 0.6090 | Test Accuracy: 79.06%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.6089787331581116, 79.06)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ======================================================================\n",
    "# SECTION 5: TRAIN + EVALUATE PIPELINE (SGD + Scheduler, batch=256)\n",
    "# ======================================================================\n",
    "\n",
    "BATCH_SIZE = 256  # Update batch size\n",
    "\n",
    "# Re-create DataLoaders with new batch size\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "    trainset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "    testset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "\n",
    "# Model\n",
    "model = CustomCNN(num_classes=NUM_CLASSES).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# SGD optimizer with momentum and weight decay\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.05, momentum=0.9, weight_decay=5e-4)\n",
    "\n",
    "# StepLR scheduler: reduce LR by 0.1 every 10 epochs\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "scheduler = StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "\n",
    "# Training function with scheduler\n",
    "def train_model_with_scheduler(model, trainloader, criterion, optimizer, scheduler, epochs=10, device='cpu'):\n",
    "    history = {'loss': [], 'acc': []}\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct_predictions = 0\n",
    "        total_samples = 0\n",
    "        \n",
    "        print(f\"\\n--- Epoch {epoch+1}/{epochs} ---\")\n",
    "        for i, (inputs, labels) in enumerate(trainloader, 0):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_samples += labels.size(0)\n",
    "            correct_predictions += (predicted == labels).sum().item()\n",
    "\n",
    "        # Step the scheduler at the end of each epoch\n",
    "        scheduler.step()\n",
    "\n",
    "        epoch_loss = running_loss / total_samples\n",
    "        epoch_acc = (correct_predictions / total_samples) * 100\n",
    "        history['loss'].append(epoch_loss)\n",
    "        history['acc'].append(epoch_acc)\n",
    "        print(f\"Epoch Summary | Loss: {epoch_loss:.4f} | Accuracy: {epoch_acc:.2f}%\")\n",
    "    \n",
    "    print('\\n--- Finished Training ---')\n",
    "    return history\n",
    "\n",
    "# Train with scheduler\n",
    "train_history = train_model_with_scheduler(model, trainloader, criterion, optimizer, scheduler,\n",
    "                                           epochs=20, device=device)\n",
    "\n",
    "# Evaluate model\n",
    "evaluate_model(model, testloader, criterion, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c4916906",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Setting up hyperparameters and device ---\n",
      "Using device: cuda:0\n",
      "\n",
      "--- Preparing CIFAR-10 dataset ---\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "\n",
      "--- Epoch 1/20 ---\n",
      "Epoch Summary | Loss: 2.7925 | Accuracy: 27.36%\n",
      "\n",
      "--- Epoch 2/20 ---\n",
      "Epoch Summary | Loss: 1.6178 | Accuracy: 41.29%\n",
      "\n",
      "--- Epoch 3/20 ---\n",
      "Epoch Summary | Loss: 1.4218 | Accuracy: 48.23%\n",
      "\n",
      "--- Epoch 4/20 ---\n",
      "Epoch Summary | Loss: 1.3317 | Accuracy: 51.85%\n",
      "\n",
      "--- Epoch 5/20 ---\n",
      "Epoch Summary | Loss: 1.2769 | Accuracy: 54.39%\n",
      "\n",
      "--- Epoch 6/20 ---\n",
      "Epoch Summary | Loss: 1.2184 | Accuracy: 56.69%\n",
      "\n",
      "--- Epoch 7/20 ---\n",
      "Epoch Summary | Loss: 1.1679 | Accuracy: 58.76%\n",
      "\n",
      "--- Epoch 8/20 ---\n",
      "Epoch Summary | Loss: 1.1360 | Accuracy: 59.82%\n",
      "\n",
      "--- Epoch 9/20 ---\n",
      "Epoch Summary | Loss: 1.1386 | Accuracy: 59.49%\n",
      "\n",
      "--- Epoch 10/20 ---\n",
      "Epoch Summary | Loss: 1.1165 | Accuracy: 60.43%\n",
      "\n",
      "--- Epoch 11/20 ---\n",
      "Epoch Summary | Loss: 1.1025 | Accuracy: 61.03%\n",
      "\n",
      "--- Epoch 12/20 ---\n",
      "Epoch Summary | Loss: 1.0922 | Accuracy: 61.30%\n",
      "\n",
      "--- Epoch 13/20 ---\n",
      "Epoch Summary | Loss: 1.0856 | Accuracy: 61.65%\n",
      "\n",
      "--- Epoch 14/20 ---\n",
      "Epoch Summary | Loss: 1.0798 | Accuracy: 61.97%\n",
      "\n",
      "--- Epoch 15/20 ---\n",
      "Epoch Summary | Loss: 1.0804 | Accuracy: 61.70%\n",
      "\n",
      "--- Epoch 16/20 ---\n",
      "Epoch Summary | Loss: 1.0659 | Accuracy: 62.46%\n",
      "\n",
      "--- Epoch 17/20 ---\n",
      "Epoch Summary | Loss: 1.0719 | Accuracy: 62.24%\n",
      "\n",
      "--- Epoch 18/20 ---\n",
      "Epoch Summary | Loss: 1.0639 | Accuracy: 62.38%\n",
      "\n",
      "--- Epoch 19/20 ---\n",
      "Epoch Summary | Loss: 1.0813 | Accuracy: 61.98%\n",
      "\n",
      "--- Epoch 20/20 ---\n",
      "Epoch Summary | Loss: 1.0583 | Accuracy: 62.44%\n",
      "\n",
      "--- Finished Training ---\n",
      "\n",
      "Test Loss: 1.0440 | Test Accuracy: 64.04%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1.043992974472046, 64.03999999999999)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ======================================================================\n",
    "# CUSTOM CNN on CIFAR-10 (PyTorch) (256 batch size)\n",
    "# ======================================================================\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F \n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# ======================================================================\n",
    "# SECTION 1: SETUP AND HYPERPARAMETERS\n",
    "# ======================================================================\n",
    "print(\"--- Setting up hyperparameters and device ---\")\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Hyperparameters\n",
    "EPOCHS = 20\n",
    "BATCH_SIZE = 256\n",
    "LEARNING_RATE = 0.001\n",
    "NUM_CLASSES = 10           # CIFAR-10 has 10 classes\n",
    "IMAGE_SIZE = 64            # Resize from 32x32 → 64x64\n",
    "\n",
    "# ======================================================================\n",
    "# SECTION 2: DATA LOADING AND AUGMENTATION\n",
    "# ======================================================================\n",
    "print(\"\\n--- Preparing CIFAR-10 dataset ---\")\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize(IMAGE_SIZE),\n",
    "    transforms.RandomCrop(IMAGE_SIZE, padding=8),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
    "                         (0.2023, 0.1994, 0.2010))\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize(IMAGE_SIZE),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
    "                         (0.2023, 0.1994, 0.2010))\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(\n",
    "    root='./data', train=True, download=True, transform=train_transform)\n",
    "testset = torchvision.datasets.CIFAR10(\n",
    "    root='./data', train=False, download=True, transform=test_transform)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "    trainset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "    testset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "\n",
    "# ======================================================================\n",
    "# SECTION 3: CUSTOM CNN MODEL DEFINITION\n",
    "# ======================================================================\n",
    "\n",
    "class CustomCNN(nn.Module):\n",
    "    \"\"\"Custom CNN architecture for CIFAR-10 (64x64).\"\"\"\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(CustomCNN, self).__init__()\n",
    "        \n",
    "        # Block 1: Conv -> BN -> ReLU -> MaxPool\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2)\n",
    "        )\n",
    "\n",
    "        # Block 2\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(16, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2)\n",
    "        )\n",
    "\n",
    "        # Block 3\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2)\n",
    "        )\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(64 * 8 * 8, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "# ======================================================================\n",
    "# SECTION 4: TRAINING AND EVALUATION FUNCTIONS\n",
    "# ======================================================================\n",
    "\n",
    "def train_model(model, trainloader, criterion, optimizer, epochs=10, device='cpu'):\n",
    "    \"\"\"Train a PyTorch model.\"\"\"\n",
    "    history = {'loss': [], 'acc': []}\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct_predictions = 0\n",
    "        total_samples = 0\n",
    "        \n",
    "        print(f\"\\n--- Epoch {epoch+1}/{epochs} ---\")\n",
    "        for i, (inputs, labels) in enumerate(trainloader, 0):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_samples += labels.size(0)\n",
    "            correct_predictions += (predicted == labels).sum().item()\n",
    "\n",
    "        epoch_loss = running_loss / total_samples\n",
    "        epoch_acc = (correct_predictions / total_samples) * 100\n",
    "        history['loss'].append(epoch_loss)\n",
    "        history['acc'].append(epoch_acc)\n",
    "        print(f\"Epoch Summary | Loss: {epoch_loss:.4f} | Accuracy: {epoch_acc:.2f}%\")\n",
    "    \n",
    "    print('\\n--- Finished Training ---')\n",
    "    return history\n",
    "\n",
    "\n",
    "def evaluate_model(model, testloader, criterion, device='cpu'):\n",
    "    \"\"\"Evaluate model performance on test set.\"\"\"\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in testloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_samples += labels.size(0)\n",
    "            correct_predictions += (predicted == labels).sum().item()\n",
    "\n",
    "    test_loss = running_loss / total_samples\n",
    "    test_acc = (correct_predictions / total_samples) * 100\n",
    "    print(f\"\\nTest Loss: {test_loss:.4f} | Test Accuracy: {test_acc:.2f}%\")\n",
    "    return test_loss, test_acc\n",
    "\n",
    "# ======================================================================\n",
    "# SECTION 5: TRAIN + EVALUATE PIPELINE (RMSProp)\n",
    "# ======================================================================\n",
    "\n",
    "model = CustomCNN(num_classes=NUM_CLASSES).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Use RMSProp optimizer\n",
    "optimizer = optim.RMSprop(model.parameters(), lr=LEARNING_RATE, alpha=0.9, weight_decay=5e-4, momentum=0.9)\n",
    "\n",
    "train_history = train_model(model, trainloader, criterion, optimizer,\n",
    "                            epochs=EPOCHS, device=device)\n",
    "evaluate_model(model, testloader, criterion, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c58c0a29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Setting up hyperparameters and device ---\n",
      "Using device: cuda:0\n",
      "\n",
      "--- Preparing CIFAR-10 dataset ---\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "\n",
      "--- Epoch 1/20 ---\n",
      "Epoch Summary | Loss: 2.8296 | Accuracy: 33.46%\n",
      "\n",
      "--- Epoch 2/20 ---\n",
      "Epoch Summary | Loss: 1.4674 | Accuracy: 46.85%\n",
      "\n",
      "--- Epoch 3/20 ---\n",
      "Epoch Summary | Loss: 1.2817 | Accuracy: 53.44%\n",
      "\n",
      "--- Epoch 4/20 ---\n",
      "Epoch Summary | Loss: 1.1758 | Accuracy: 57.87%\n",
      "\n",
      "--- Epoch 5/20 ---\n",
      "Epoch Summary | Loss: 1.1191 | Accuracy: 59.97%\n",
      "\n",
      "--- Epoch 6/20 ---\n",
      "Epoch Summary | Loss: 1.0554 | Accuracy: 62.18%\n",
      "\n",
      "--- Epoch 7/20 ---\n",
      "Epoch Summary | Loss: 1.0168 | Accuracy: 64.10%\n",
      "\n",
      "--- Epoch 8/20 ---\n",
      "Epoch Summary | Loss: 0.9869 | Accuracy: 65.08%\n",
      "\n",
      "--- Epoch 9/20 ---\n",
      "Epoch Summary | Loss: 0.9679 | Accuracy: 65.87%\n",
      "\n",
      "--- Epoch 10/20 ---\n",
      "Epoch Summary | Loss: 0.9535 | Accuracy: 66.37%\n",
      "\n",
      "--- Epoch 11/20 ---\n",
      "Epoch Summary | Loss: 0.9440 | Accuracy: 66.90%\n",
      "\n",
      "--- Epoch 12/20 ---\n",
      "Epoch Summary | Loss: 0.9345 | Accuracy: 66.95%\n",
      "\n",
      "--- Epoch 13/20 ---\n",
      "Epoch Summary | Loss: 0.9346 | Accuracy: 67.31%\n",
      "\n",
      "--- Epoch 14/20 ---\n",
      "Epoch Summary | Loss: 0.9264 | Accuracy: 67.51%\n",
      "\n",
      "--- Epoch 15/20 ---\n",
      "Epoch Summary | Loss: 0.9176 | Accuracy: 67.76%\n",
      "\n",
      "--- Epoch 16/20 ---\n",
      "Epoch Summary | Loss: 0.9144 | Accuracy: 67.70%\n",
      "\n",
      "--- Epoch 17/20 ---\n",
      "Epoch Summary | Loss: 0.9060 | Accuracy: 68.14%\n",
      "\n",
      "--- Epoch 18/20 ---\n",
      "Epoch Summary | Loss: 0.9032 | Accuracy: 68.38%\n",
      "\n",
      "--- Epoch 19/20 ---\n",
      "Epoch Summary | Loss: 0.9029 | Accuracy: 68.37%\n",
      "\n",
      "--- Epoch 20/20 ---\n",
      "Epoch Summary | Loss: 0.8981 | Accuracy: 68.52%\n",
      "\n",
      "--- Finished Training ---\n",
      "\n",
      "Test Loss: 1.2682 | Test Accuracy: 60.01%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1.268196096420288, 60.01)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ======================================================================\n",
    "# CUSTOM CNN on CIFAR-10 (PyTorch) (256 batch size)\n",
    "# ======================================================================\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F \n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# ======================================================================\n",
    "# SECTION 1: SETUP AND HYPERPARAMETERS\n",
    "# ======================================================================\n",
    "print(\"--- Setting up hyperparameters and device ---\")\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Hyperparameters\n",
    "EPOCHS = 20\n",
    "BATCH_SIZE = 256\n",
    "LEARNING_RATE = 0.001\n",
    "NUM_CLASSES = 10           # CIFAR-10 has 10 classes\n",
    "IMAGE_SIZE = 64            # Resize from 32x32 → 64x64\n",
    "\n",
    "# ======================================================================\n",
    "# SECTION 2: DATA LOADING AND AUGMENTATION\n",
    "# ======================================================================\n",
    "print(\"\\n--- Preparing CIFAR-10 dataset ---\")\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize(IMAGE_SIZE),\n",
    "    transforms.RandomCrop(IMAGE_SIZE, padding=8),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
    "                         (0.2023, 0.1994, 0.2010))\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize(IMAGE_SIZE),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
    "                         (0.2023, 0.1994, 0.2010))\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(\n",
    "    root='./data', train=True, download=True, transform=train_transform)\n",
    "testset = torchvision.datasets.CIFAR10(\n",
    "    root='./data', train=False, download=True, transform=test_transform)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "    trainset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "    testset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "\n",
    "# ======================================================================\n",
    "# SECTION 3: CUSTOM CNN MODEL DEFINITION\n",
    "# ======================================================================\n",
    "\n",
    "class CustomCNN(nn.Module):\n",
    "    \"\"\"Custom CNN architecture for CIFAR-10 (64x64).\"\"\"\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(CustomCNN, self).__init__()\n",
    "        \n",
    "        # Block 1: Conv -> BN -> ReLU -> MaxPool\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2)\n",
    "        )\n",
    "\n",
    "        # Block 2\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(16, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2)\n",
    "        )\n",
    "\n",
    "        # Block 3\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2)\n",
    "        )\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(64 * 8 * 8, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "# ======================================================================\n",
    "# SECTION 4: TRAINING AND EVALUATION FUNCTIONS\n",
    "# ======================================================================\n",
    "\n",
    "def train_model(model, trainloader, criterion, optimizer, epochs=10, device='cpu'):\n",
    "    \"\"\"Train a PyTorch model.\"\"\"\n",
    "    history = {'loss': [], 'acc': []}\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct_predictions = 0\n",
    "        total_samples = 0\n",
    "        \n",
    "        print(f\"\\n--- Epoch {epoch+1}/{epochs} ---\")\n",
    "        for i, (inputs, labels) in enumerate(trainloader, 0):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_samples += labels.size(0)\n",
    "            correct_predictions += (predicted == labels).sum().item()\n",
    "\n",
    "        epoch_loss = running_loss / total_samples\n",
    "        epoch_acc = (correct_predictions / total_samples) * 100\n",
    "        history['loss'].append(epoch_loss)\n",
    "        history['acc'].append(epoch_acc)\n",
    "        print(f\"Epoch Summary | Loss: {epoch_loss:.4f} | Accuracy: {epoch_acc:.2f}%\")\n",
    "    \n",
    "    print('\\n--- Finished Training ---')\n",
    "    return history\n",
    "\n",
    "\n",
    "def evaluate_model(model, testloader, criterion, device='cpu'):\n",
    "    \"\"\"Evaluate model performance on test set.\"\"\"\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in testloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_samples += labels.size(0)\n",
    "            correct_predictions += (predicted == labels).sum().item()\n",
    "\n",
    "    test_loss = running_loss / total_samples\n",
    "    test_acc = (correct_predictions / total_samples) * 100\n",
    "    print(f\"\\nTest Loss: {test_loss:.4f} | Test Accuracy: {test_acc:.2f}%\")\n",
    "    return test_loss, test_acc\n",
    "\n",
    "# ======================================================================\n",
    "# SECTION 5: TRAIN + EVALUATE PIPELINE (RMSProp)\n",
    "# ======================================================================\n",
    "\n",
    "model = CustomCNN(num_classes=NUM_CLASSES).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Use RMSProp optimizer\n",
    "optimizer = optim.RMSprop(model.parameters(), lr=LEARNING_RATE, alpha=0.9, weight_decay=5e-4, momentum=0.9)\n",
    "\n",
    "train_history = train_model(model, trainloader, criterion, optimizer,\n",
    "                            epochs=EPOCHS, device=device)\n",
    "evaluate_model(model, testloader, criterion, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "36a5ff9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Setting up hyperparameters and device ---\n",
      "Using device: cuda:0\n",
      "\n",
      "--- Preparing CIFAR-10 dataset ---\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "\n",
      "--- Epoch 1/20 ---\n",
      "Epoch Summary | Loss: 1.5392 | Accuracy: 44.02%\n",
      "\n",
      "--- Epoch 2/20 ---\n",
      "Epoch Summary | Loss: 1.2462 | Accuracy: 55.27%\n",
      "\n",
      "--- Epoch 3/20 ---\n",
      "Epoch Summary | Loss: 1.1152 | Accuracy: 60.24%\n",
      "\n",
      "--- Epoch 4/20 ---\n",
      "Epoch Summary | Loss: 1.0248 | Accuracy: 63.44%\n",
      "\n",
      "--- Epoch 5/20 ---\n",
      "Epoch Summary | Loss: 0.9604 | Accuracy: 66.17%\n",
      "\n",
      "--- Epoch 6/20 ---\n",
      "Epoch Summary | Loss: 0.9040 | Accuracy: 68.12%\n",
      "\n",
      "--- Epoch 7/20 ---\n",
      "Epoch Summary | Loss: 0.8604 | Accuracy: 69.60%\n",
      "\n",
      "--- Epoch 8/20 ---\n",
      "Epoch Summary | Loss: 0.8188 | Accuracy: 70.99%\n",
      "\n",
      "--- Epoch 9/20 ---\n",
      "Epoch Summary | Loss: 0.7981 | Accuracy: 71.97%\n",
      "\n",
      "--- Epoch 10/20 ---\n",
      "Epoch Summary | Loss: 0.7762 | Accuracy: 72.67%\n",
      "\n",
      "--- Epoch 11/20 ---\n",
      "Epoch Summary | Loss: 0.7418 | Accuracy: 74.16%\n",
      "\n",
      "--- Epoch 12/20 ---\n",
      "Epoch Summary | Loss: 0.7219 | Accuracy: 74.46%\n",
      "\n",
      "--- Epoch 13/20 ---\n",
      "Epoch Summary | Loss: 0.7029 | Accuracy: 75.25%\n",
      "\n",
      "--- Epoch 14/20 ---\n",
      "Epoch Summary | Loss: 0.6869 | Accuracy: 75.81%\n",
      "\n",
      "--- Epoch 15/20 ---\n",
      "Epoch Summary | Loss: 0.6700 | Accuracy: 76.41%\n",
      "\n",
      "--- Epoch 16/20 ---\n",
      "Epoch Summary | Loss: 0.6545 | Accuracy: 76.92%\n",
      "\n",
      "--- Epoch 17/20 ---\n",
      "Epoch Summary | Loss: 0.6428 | Accuracy: 77.39%\n",
      "\n",
      "--- Epoch 18/20 ---\n",
      "Epoch Summary | Loss: 0.6266 | Accuracy: 77.97%\n",
      "\n",
      "--- Epoch 19/20 ---\n",
      "Epoch Summary | Loss: 0.6138 | Accuracy: 78.25%\n",
      "\n",
      "--- Epoch 20/20 ---\n",
      "Epoch Summary | Loss: 0.6139 | Accuracy: 78.28%\n",
      "\n",
      "--- Finished Training ---\n",
      "\n",
      "Test Loss: 0.6181 | Test Accuracy: 78.32%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.6180855454444886, 78.32000000000001)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ======================================================================\n",
    "# CUSTOM CNN on CIFAR-10 (PyTorch) (256 batch size)\n",
    "# ======================================================================\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F \n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# ======================================================================\n",
    "# SECTION 1: SETUP AND HYPERPARAMETERS\n",
    "# ======================================================================\n",
    "print(\"--- Setting up hyperparameters and device ---\")\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Hyperparameters\n",
    "EPOCHS = 20\n",
    "BATCH_SIZE = 256\n",
    "LEARNING_RATE = 0.0005\n",
    "NUM_CLASSES = 10           # CIFAR-10 has 10 classes\n",
    "IMAGE_SIZE = 64            # Resize from 32x32 → 64x64\n",
    "\n",
    "# ======================================================================\n",
    "# SECTION 2: DATA LOADING AND AUGMENTATION\n",
    "# ======================================================================\n",
    "print(\"\\n--- Preparing CIFAR-10 dataset ---\")\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize(IMAGE_SIZE),\n",
    "    transforms.RandomCrop(IMAGE_SIZE, padding=8),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
    "                         (0.2023, 0.1994, 0.2010))\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize(IMAGE_SIZE),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
    "                         (0.2023, 0.1994, 0.2010))\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(\n",
    "    root='./data', train=True, download=True, transform=train_transform)\n",
    "testset = torchvision.datasets.CIFAR10(\n",
    "    root='./data', train=False, download=True, transform=test_transform)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "    trainset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "    testset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "\n",
    "# ======================================================================\n",
    "# SECTION 3: CUSTOM CNN MODEL DEFINITION\n",
    "# ======================================================================\n",
    "\n",
    "class CustomCNN(nn.Module):\n",
    "    \"\"\"Custom CNN architecture for CIFAR-10 (64x64).\"\"\"\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(CustomCNN, self).__init__()\n",
    "        \n",
    "        # Block 1: Conv -> BN -> ReLU -> MaxPool\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2)\n",
    "        )\n",
    "\n",
    "        # Block 2\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(16, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2)\n",
    "        )\n",
    "\n",
    "        # Block 3\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2)\n",
    "        )\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(64 * 8 * 8, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "# ======================================================================\n",
    "# SECTION 4: TRAINING AND EVALUATION FUNCTIONS\n",
    "# ======================================================================\n",
    "\n",
    "def train_model(model, trainloader, criterion, optimizer, epochs=10, device='cpu'):\n",
    "    \"\"\"Train a PyTorch model.\"\"\"\n",
    "    history = {'loss': [], 'acc': []}\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct_predictions = 0\n",
    "        total_samples = 0\n",
    "        \n",
    "        print(f\"\\n--- Epoch {epoch+1}/{epochs} ---\")\n",
    "        for i, (inputs, labels) in enumerate(trainloader, 0):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_samples += labels.size(0)\n",
    "            correct_predictions += (predicted == labels).sum().item()\n",
    "\n",
    "        epoch_loss = running_loss / total_samples\n",
    "        epoch_acc = (correct_predictions / total_samples) * 100\n",
    "        history['loss'].append(epoch_loss)\n",
    "        history['acc'].append(epoch_acc)\n",
    "        print(f\"Epoch Summary | Loss: {epoch_loss:.4f} | Accuracy: {epoch_acc:.2f}%\")\n",
    "    \n",
    "    print('\\n--- Finished Training ---')\n",
    "    return history\n",
    "\n",
    "\n",
    "def evaluate_model(model, testloader, criterion, device='cpu'):\n",
    "    \"\"\"Evaluate model performance on test set.\"\"\"\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in testloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_samples += labels.size(0)\n",
    "            correct_predictions += (predicted == labels).sum().item()\n",
    "\n",
    "    test_loss = running_loss / total_samples\n",
    "    test_acc = (correct_predictions / total_samples) * 100\n",
    "    print(f\"\\nTest Loss: {test_loss:.4f} | Test Accuracy: {test_acc:.2f}%\")\n",
    "    return test_loss, test_acc\n",
    "\n",
    "# ======================================================================\n",
    "# SECTION 5: TRAIN + EVALUATE PIPELINE\n",
    "# ======================================================================\n",
    "\n",
    "model = CustomCNN(num_classes=NUM_CLASSES).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "train_history = train_model(model, trainloader, criterion, optimizer,\n",
    "                            epochs=EPOCHS, device=device)\n",
    "evaluate_model(model, testloader, criterion, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4e605ec9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Epoch 1/20 ---\n",
      "Epoch Summary | Loss: 1.9146 | Accuracy: 31.53%\n",
      "\n",
      "--- Epoch 2/20 ---\n",
      "Epoch Summary | Loss: 1.5725 | Accuracy: 43.33%\n",
      "\n",
      "--- Epoch 3/20 ---\n",
      "Epoch Summary | Loss: 1.4489 | Accuracy: 47.30%\n",
      "\n",
      "--- Epoch 4/20 ---\n",
      "Epoch Summary | Loss: 1.3763 | Accuracy: 50.49%\n",
      "\n",
      "--- Epoch 5/20 ---\n",
      "Epoch Summary | Loss: 1.3211 | Accuracy: 52.83%\n",
      "\n",
      "--- Epoch 6/20 ---\n",
      "Epoch Summary | Loss: 1.2662 | Accuracy: 54.73%\n",
      "\n",
      "--- Epoch 7/20 ---\n",
      "Epoch Summary | Loss: 1.2233 | Accuracy: 56.50%\n",
      "\n",
      "--- Epoch 8/20 ---\n",
      "Epoch Summary | Loss: 1.1782 | Accuracy: 58.25%\n",
      "\n",
      "--- Epoch 9/20 ---\n",
      "Epoch Summary | Loss: 1.1465 | Accuracy: 59.50%\n",
      "\n",
      "--- Epoch 10/20 ---\n",
      "Epoch Summary | Loss: 1.1150 | Accuracy: 60.51%\n",
      "\n",
      "--- Epoch 11/20 ---\n",
      "Epoch Summary | Loss: 1.0687 | Accuracy: 62.60%\n",
      "\n",
      "--- Epoch 12/20 ---\n",
      "Epoch Summary | Loss: 1.0696 | Accuracy: 62.44%\n",
      "\n",
      "--- Epoch 13/20 ---\n",
      "Epoch Summary | Loss: 1.0628 | Accuracy: 62.64%\n",
      "\n",
      "--- Epoch 14/20 ---\n",
      "Epoch Summary | Loss: 1.0579 | Accuracy: 62.81%\n",
      "\n",
      "--- Epoch 15/20 ---\n",
      "Epoch Summary | Loss: 1.0518 | Accuracy: 63.11%\n",
      "\n",
      "--- Epoch 16/20 ---\n",
      "Epoch Summary | Loss: 1.0537 | Accuracy: 62.94%\n",
      "\n",
      "--- Epoch 17/20 ---\n",
      "Epoch Summary | Loss: 1.0480 | Accuracy: 63.35%\n",
      "\n",
      "--- Epoch 18/20 ---\n",
      "Epoch Summary | Loss: 1.0476 | Accuracy: 63.34%\n",
      "\n",
      "--- Epoch 19/20 ---\n",
      "Epoch Summary | Loss: 1.0446 | Accuracy: 63.45%\n",
      "\n",
      "--- Epoch 20/20 ---\n",
      "Epoch Summary | Loss: 1.0419 | Accuracy: 63.49%\n",
      "\n",
      "--- Finished Training ---\n",
      "\n",
      "Test Loss: 0.9589 | Test Accuracy: 66.64%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9588596880912781, 66.64)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ======================================================================\n",
    "# SECTION 5: TRAIN + EVALUATE PIPELINE (SGD + Scheduler, batch=256)\n",
    "# ======================================================================\n",
    "\n",
    "BATCH_SIZE = 256  # Update batch size\n",
    "\n",
    "# Re-create DataLoaders with new batch size\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "    trainset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "    testset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "\n",
    "# Model\n",
    "model = CustomCNN(num_classes=NUM_CLASSES).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# SGD optimizer with momentum and weight decay\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9, weight_decay=5e-4)\n",
    "\n",
    "# StepLR scheduler: reduce LR by 0.1 every 10 epochs\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "scheduler = StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "\n",
    "# Training function with scheduler\n",
    "def train_model_with_scheduler(model, trainloader, criterion, optimizer, scheduler, epochs=10, device='cpu'):\n",
    "    history = {'loss': [], 'acc': []}\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct_predictions = 0\n",
    "        total_samples = 0\n",
    "        \n",
    "        print(f\"\\n--- Epoch {epoch+1}/{epochs} ---\")\n",
    "        for i, (inputs, labels) in enumerate(trainloader, 0):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_samples += labels.size(0)\n",
    "            correct_predictions += (predicted == labels).sum().item()\n",
    "\n",
    "        # Step the scheduler at the end of each epoch\n",
    "        scheduler.step()\n",
    "\n",
    "        epoch_loss = running_loss / total_samples\n",
    "        epoch_acc = (correct_predictions / total_samples) * 100\n",
    "        history['loss'].append(epoch_loss)\n",
    "        history['acc'].append(epoch_acc)\n",
    "        print(f\"Epoch Summary | Loss: {epoch_loss:.4f} | Accuracy: {epoch_acc:.2f}%\")\n",
    "    \n",
    "    print('\\n--- Finished Training ---')\n",
    "    return history\n",
    "\n",
    "# Train with scheduler\n",
    "train_history = train_model_with_scheduler(model, trainloader, criterion, optimizer, scheduler,\n",
    "                                           epochs=20, device=device)\n",
    "\n",
    "# Evaluate model\n",
    "evaluate_model(model, testloader, criterion, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cc5dbe38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Epoch 1/20 ---\n",
      "Epoch Summary | Loss: 1.5937 | Accuracy: 41.50%\n",
      "\n",
      "--- Epoch 2/20 ---\n",
      "Epoch Summary | Loss: 1.2696 | Accuracy: 53.61%\n",
      "\n",
      "--- Epoch 3/20 ---\n",
      "Epoch Summary | Loss: 1.0955 | Accuracy: 60.68%\n",
      "\n",
      "--- Epoch 4/20 ---\n",
      "Epoch Summary | Loss: 1.0090 | Accuracy: 63.83%\n",
      "\n",
      "--- Epoch 5/20 ---\n",
      "Epoch Summary | Loss: 0.9226 | Accuracy: 67.23%\n",
      "\n",
      "--- Epoch 6/20 ---\n",
      "Epoch Summary | Loss: 0.8718 | Accuracy: 69.02%\n",
      "\n",
      "--- Epoch 7/20 ---\n",
      "Epoch Summary | Loss: 0.8225 | Accuracy: 70.82%\n",
      "\n",
      "--- Epoch 8/20 ---\n",
      "Epoch Summary | Loss: 0.7812 | Accuracy: 72.54%\n",
      "\n",
      "--- Epoch 9/20 ---\n",
      "Epoch Summary | Loss: 0.7479 | Accuracy: 73.74%\n",
      "\n",
      "--- Epoch 10/20 ---\n",
      "Epoch Summary | Loss: 0.7205 | Accuracy: 74.75%\n",
      "\n",
      "--- Epoch 11/20 ---\n",
      "Epoch Summary | Loss: 0.6986 | Accuracy: 75.28%\n",
      "\n",
      "--- Epoch 12/20 ---\n",
      "Epoch Summary | Loss: 0.6709 | Accuracy: 76.39%\n",
      "\n",
      "--- Epoch 13/20 ---\n",
      "Epoch Summary | Loss: 0.6442 | Accuracy: 77.45%\n",
      "\n",
      "--- Epoch 14/20 ---\n",
      "Epoch Summary | Loss: 0.6302 | Accuracy: 77.82%\n",
      "\n",
      "--- Epoch 15/20 ---\n",
      "Epoch Summary | Loss: 0.6146 | Accuracy: 78.50%\n",
      "\n",
      "--- Epoch 16/20 ---\n",
      "Epoch Summary | Loss: 0.5947 | Accuracy: 79.05%\n",
      "\n",
      "--- Epoch 17/20 ---\n",
      "Epoch Summary | Loss: 0.5801 | Accuracy: 79.67%\n",
      "\n",
      "--- Epoch 18/20 ---\n",
      "Epoch Summary | Loss: 0.5643 | Accuracy: 80.18%\n",
      "\n",
      "--- Epoch 19/20 ---\n",
      "Epoch Summary | Loss: 0.5543 | Accuracy: 80.58%\n",
      "\n",
      "--- Epoch 20/20 ---\n",
      "Epoch Summary | Loss: 0.5423 | Accuracy: 81.07%\n",
      "\n",
      "--- Finished Training ---\n",
      "\n",
      "Test Loss: 0.5634 | Test Accuracy: 80.64%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.5634178160667419, 80.64)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "EPOCHS = 20\n",
    "BATCH_SIZE = 256\n",
    "LEARNING_RATE = 0.001\n",
    "NUM_CLASSES = 10           # CIFAR-10 has 10 classes\n",
    "IMAGE_SIZE = 64            # Resize from 32x32 → 64x64\n",
    "# ======================================================================\n",
    "# SECTION 5: TRAIN + EVALUATE PIPELINE\n",
    "# ======================================================================\n",
    "\n",
    "model = CustomCNN(num_classes=NUM_CLASSES).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=5e-4)\n",
    "\n",
    "\n",
    "train_history = train_model(model, trainloader, criterion, optimizer,\n",
    "                            epochs=EPOCHS, device=device)\n",
    "evaluate_model(model, testloader, criterion, device=device)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.11.8)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
